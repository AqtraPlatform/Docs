#!/usr/bin/env python3
"""
Normalize very long URLs in Markdown files to reference-style links
"""

import re
import csv
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
import argparse

class LinkNormalizer:
    def __init__(self, src_dir: str = "docs", threshold: int = 120):
        self.src_dir = Path(src_dir).resolve()
        self.threshold = threshold
        self.normalized_links = []
        self.skipped_links = []
        
    def normalize_links(self, dry_run: bool = False) -> bool:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        print("üîó –ó–∞–ø—É—Å–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
        print(f"  üìÅ –ò—Å—Ö–æ–¥–Ω–∏–∫–∏: {self.src_dir}")
        print(f"  üìè –ü–æ—Ä–æ–≥ –¥–ª–∏–Ω—ã: {self.threshold} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if not self.src_dir.exists():
            print("‚ùå –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω–∏–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ .md —Ñ–∞–π–ª—ã
        md_files = list(self.src_dir.rglob('*.md'))
        print(f"  üìÅ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} Markdown —Ñ–∞–π–ª–æ–≤")
        
        for md_file in md_files:
            self._normalize_file_links(md_file, dry_run)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç—ã
        self._save_reports()
        
        # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
        self._print_report()
        
        return True
    
    def _normalize_file_links(self, md_file: Path, dry_run: bool):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ"""
        try:
            content = md_file.read_text(encoding='utf-8')
            original_content = content
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
            link_patterns = [
                # [text](url) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π markdown
                r'\[([^\]]+)\]\(([^)]+)\)',
                # [text](url "title") - —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                r'\[([^\]]+)\]\(([^)]+)\s+"([^"]+)"\)',
            ]
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            links_to_normalize = []
            reference_definitions = {}
            
            for pattern in link_patterns:
                for match in re.finditer(pattern, content):
                    text = match.group(1)
                    url = match.group(2)
                    title = match.group(3) if len(match.groups()) > 2 else None
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É URL
                    if len(url) > self.threshold and not url.startswith(('http://', 'https://')):
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                        continue
                    
                    if len(url) > self.threshold:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É-—Å—Å—ã–ª–∫—É
                        ref_key = self._generate_reference_key(text, url)
                        if ref_key not in reference_definitions:
                            reference_definitions[ref_key] = {
                                'url': url,
                                'title': title,
                                'original_match': match.group(0)
                            }
                            links_to_normalize.append({
                                'text': text,
                                'url': url,
                                'title': title,
                                'ref_key': ref_key,
                                'original_match': match.group(0)
                            })
            
            if not links_to_normalize:
                return
            
            # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Å—ã–ª–∫–∏-—Å—Å—ã–ª–∫–∏
            new_content = content
            for link in links_to_normalize:
                if link['title']:
                    new_ref = f"[{link['text']}][{link['ref_key']}]"
                else:
                    new_ref = f"[{link['text']}][{link['ref_key']}]"
                
                new_content = new_content.replace(link['original_match'], new_ref)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
            if reference_definitions:
                ref_section = self._build_reference_section(reference_definitions)
                new_content = new_content.rstrip() + "\n\n" + ref_section + "\n"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if not dry_run and new_content != original_content:
                md_file.write_text(new_content, encoding='utf-8')
                print(f"  ‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: {md_file.relative_to(self.src_dir)}")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ—Ç—á–µ—Ç
            for link in links_to_normalize:
                self.normalized_links.append({
                    'page': str(md_file.relative_to(self.src_dir)),
                    'original': link['original_match'],
                    'replacement': f"[{link['text']}][{link['ref_key']}]",
                    'ref_key': link['ref_key'],
                    'url': link['url'],
                    'status': 'normalized'
                })
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {md_file}: {e}")
            self.skipped_links.append({
                'page': str(md_file.relative_to(self.src_dir)),
                'original': '',
                'replacement': '',
                'ref_key': '',
                'url': '',
                'status': 'skipped'
            })
    
    def _generate_reference_key(self, text: str, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è —Å—Å—ã–ª–∫–∏-—Å—Å—ã–ª–∫–∏"""
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
        key = re.sub(r'[^\w\s-]', '', text.lower())
        key = re.sub(r'\s+', '-', key)
        key = key[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        counter = 1
        original_key = key
        while f"[{key}]" in self._get_existing_references():
            key = f"{original_key}-{counter}"
            counter += 1
        
        return key
    
    def _get_existing_references(self) -> Set[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏-—Å—Å—ã–ª–∫–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
        existing = set()
        for md_file in self.src_dir.rglob('*.md'):
            try:
                content = md_file.read_text(encoding='utf-8')
                # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏-—Å—Å—ã–ª–∫–∏
                refs = re.findall(r'\[([^\]]+)\]', content)
                existing.update(refs)
            except Exception:
                continue
        return existing
    
    def _build_reference_section(self, reference_definitions: Dict) -> str:
        """–°—Ç—Ä–æ–∏—Ç —Å–µ–∫—Ü–∏—é —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ —Å—Å—ã–ª–æ–∫"""
        section = "<!-- Reference links -->\n"
        for ref_key, ref_data in reference_definitions.items():
            if ref_data['title']:
                section += f"[{ref_key}]: {ref_data['url']} \"{ref_data['title']}\"\n"
            else:
                section += f"[{ref_key}]: {ref_data['url']}\n"
        return section
    
    def _save_reports(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –≤ CSV"""
        build_dir = Path("build")
        build_dir.mkdir(exist_ok=True)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
        all_records = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        for link in self.normalized_links:
            all_records.append({
                'page': link['page'],
                'original': link['original'],
                'replacement': link['replacement'],
                'ref_key': link['ref_key'],
                'url': link['url'],
                'status': link['status']
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        for link in self.skipped_links:
            all_records.append({
                'page': link['page'],
                'original': link['original'],
                'replacement': link['replacement'],
                'ref_key': link['ref_key'],
                'url': link['url'],
                'status': link['status']
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        with open(build_dir / "links_normalized.csv", "w", newline="", encoding="utf-8") as f:
            fieldnames = ['page', 'original', 'replacement', 'ref_key', 'url', 'status']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for record in all_records:
                writer.writerow(record)
    
    def _print_report(self):
        """–í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "="*60)
        print("üìã –û–¢–ß–ï–¢ –û –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò –°–°–´–õ–û–ö")
        print("="*60)
        
        if self.normalized_links:
            print(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(self.normalized_links)}")
            for link in self.normalized_links[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"  {link['page']}: {link['ref_key']} ‚Üí {link['url'][:50]}...")
            if len(self.normalized_links) > 10:
                print(f"  ... –∏ –µ—â–µ {len(self.normalized_links) - 10} —Å—Å—ã–ª–æ–∫")
        else:
            print("‚ÑπÔ∏è  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        
        if self.skipped_links:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(self.skipped_links)}")
        
        print(f"\nüìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: build/links_normalized.csv")
        print("="*60)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Normalize very long URLs in Markdown files')
    parser.add_argument('--src', default='docs', help='Source directory (default: docs)')
    parser.add_argument('--threshold', type=int, default=120, help='URL length threshold (default: 120)')
    parser.add_argument('--dry-run', action='store_true', help='Only generate report, do not apply fixes')
    parser.add_argument('--apply', action='store_true', help='Apply fixes')
    
    args = parser.parse_args()
    
    if args.apply and args.dry_run:
        print("‚ùå –ù–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å --apply –∏ --dry-run –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ")
        sys.exit(1)
    
    if not args.apply and not args.dry_run:
        print("‚ÑπÔ∏è  –†–µ–∂–∏–º –Ω–µ —É–∫–∞–∑–∞–Ω. –ó–∞–ø—É—Å–∫ –≤ —Ä–µ–∂–∏–º–µ --dry-run")
        args.dry_run = True
    
    normalizer = LinkNormalizer(args.src, args.threshold)
    success = normalizer.normalize_links(args.dry_run)
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main()
